<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>PPforest • PPforest</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="PPforest">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-primary" data-bs-theme="dark" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">PPforest</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.2.0</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/PPforest-vignette.html">PPforest</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="nav-link" href="https://github.com/natydasilva/PPforest/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>PPforest</h1>
                        <h4 data-toc-skip class="author">N. da Silva, D.
Cook &amp; E.K Lee</h4>
            
            <h4 data-toc-skip class="date">2025-07-23</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/natydasilva/PPforest/blob/HEAD/vignettes/PPforest-vignette.Rmd"><code>vignettes/PPforest-vignette.Rmd</code></a></small>
      <div class="d-none name"><code>PPforest-vignette.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>The <code>PPforest</code> package (projection pursuit random forest)
contains functions to fit a projection pursuit random forest for
classification problems described in <span class="citation">(da Silva,
Cook, and Lee 2021)</span>. This method utilize combinations of
variables in each tree construction. In a random forest each split is
based on a single variable, chosen from a subset of predictors. In the
<code>PPforest</code>, each split is based on a linear combination of
randomly chosen variables. The linear combination is computed by
optimizing a projection pursuit index, to get a projection of the
variables that best separates the classes. The <code>PPforest</code>
uses the <code>PPtree</code> algorithm <span class="citation">(Y. D. Lee
et al. 2013)</span>, which fits a single tree to the data. Utilizing
linear combinations of variables to separate classes takes the
correlation between variables into account, and can outperform the basic
forest when separations between groups occurs on combinations of
variables. Two projection pursuit indexes, LDA and PDA, are used for
<code>PPforest</code>.</p>
<p>To improve the speed performance <code>PPforest</code> package,
<code>PPtree</code> algorithm was translated to Rcpp.
<code>PPforest</code> package utilizes a number of R packages some of
them included in “suggests” not to load them all at package
start-up.</p>
<p>You can install the package from CRAN:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">install.package</span><span class="op">(</span><span class="va">PPforest</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/natydasilva/PPforest">PPforest</a></span><span class="op">)</span></span></code></pre></div>
<p>Or the development version of <code>PPforest</code> can be installed
from github using:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://devtools.r-lib.org/" class="external-link">devtools</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://remotes.r-lib.org/reference/install_github.html" class="external-link">install_github</a></span><span class="op">(</span><span class="st">"natydasilva/PPforest"</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/natydasilva/PPforest">PPforest</a></span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="projection-pursuit-classification-forest">Projection pursuit classification forest<a class="anchor" aria-label="anchor" href="#projection-pursuit-classification-forest"></a>
</h2>
<p>In <code>PPforest</code>, projection pursuit classification trees are
used as the individual model to be combined in the forest. The original
algorithm is in <code>PPtreeViz</code> package, we translate the
original tree algorithm into <code>Rcpp</code> to improve the speed
performance to run the forest.</p>
<p>One important characteristic of PPtree is that treats the data always
as a two-class system, when the classes are more than two the algorithm
uses a two step projection pursuits optimization in every node split.
Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(X_i,y_i)</annotation></semantics></math>
the data set,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding="application/x-tex">X_i</annotation></semantics></math>
is a p-dimensional vector of explanatory variables and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>∈</mo><mrow><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi>…</mi><mi>G</mi></mrow></mrow><annotation encoding="application/x-tex">y_i\in {1,2,\ldots G}</annotation></semantics></math>
represents class information with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>…</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">i=1,\ldots n</annotation></semantics></math>.</p>
<p>In the first step optimize a projection pursuit index to find an
optimal one-dimension projection
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>α</mi><mo>*</mo></msup><annotation encoding="application/x-tex">\alpha^*</annotation></semantics></math>
for separating all classes in the current data. With the projected data
redefine the problem in a two class problem by comparing means, and
assign a new label
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">G1</annotation></semantics></math>
or
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">G2</annotation></semantics></math>
to each observation, a new variable
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>y</mi><mi>i</mi><mo>*</mo></msubsup><annotation encoding="application/x-tex">y_i^*</annotation></semantics></math>
is created. The new groups
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">G1</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">G2</annotation></semantics></math>
can contain more than one original classes. Next step is to find an
optimal one-dimensional projection
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>,
using
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo>,</mo><msubsup><mi>y</mi><mi>i</mi><mo>*</mo></msubsup><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(X_i,y_i^*)</annotation></semantics></math>
to separate the two class problem
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">G1</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">G2</annotation></semantics></math>.
The best separation of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">G1</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">G2</annotation></semantics></math>
is determine in this step and the decision rule is defined for the
current node, if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></msubsup><msub><mi>α</mi><mi>i</mi></msub><mi>M</mi><mn>1</mn><mo>&lt;</mo><mi>c</mi></mrow><annotation encoding="application/x-tex">\sum_{i=1}^p \alpha_i M1&lt; c</annotation></semantics></math>
then assign
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">G1</annotation></semantics></math>
to the left node else assign
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">G2</annotation></semantics></math>
to the right node, where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">M1</annotation></semantics></math>
is the mean of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">G1</annotation></semantics></math>.
For each groups we can repeat all the previous steps until
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">G1</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">G2</annotation></semantics></math>
have only one class from the original classes. Base on this process to
grow the tree, the depth of PPtree is at most the number of classes
because one class is assigned only to one final node.</p>
<p>Trees from <code>PPtree</code> algorithm are simple, they use the
association between variables to find separation. If a linear boundary
exists, <code>PPtree</code> produces a tree without
misclassification.</p>
<p>Projection pursuit random forest algorithm description</p>
<ol style="list-style-type: decimal">
<li><p>Let N the number of cases in the training set
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Θ</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Theta=(X,Y)</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>
bootstrap samples from the training set are taking (samples of size N
with replacement).</p></li>
<li><p>For each bootstrap sample a \verb PPtree is grown to the largest
extent possible
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><msub><mi>Θ</mi><mi>k</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">h(x, {\Theta_k})</annotation></semantics></math>.
No pruning. This tree is grown using step 3 modification.</p></li>
<li><p>Let M the number of input variables, a number of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>&lt;</mo><mo>&lt;</mo><mi>M</mi></mrow><annotation encoding="application/x-tex">m&lt;&lt;M</annotation></semantics></math>
variables are selected at random at each node and the best split based
on a linear combination of these randomly chosen variables. The linear
combination is computed by optimizing a projection pursuit index, to get
a projection of the variables that best separates the classes.</p></li>
<li><p>Predict the classes of each case not included in the bootstrap
sample and compute oob error.</p></li>
<li><p>Based on majority vote predict the class for new data.</p></li>
</ol>
<div class="section level3">
<h3 id="overview-ppforest-package">Overview PPforest package<a class="anchor" aria-label="anchor" href="#overview-ppforest-package"></a>
</h3>
<p><code>PPforest</code> package implements a classification random
forest using projection pursuit classification trees. The following
table present all the functions in <code>PPforest</code> package.</p>
<table class="table">
<colgroup>
<col width="21%">
<col width="78%">
</colgroup>
<thead><tr class="header">
<th>Function</th>
<th>Description</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>node_data</td>
<td>Data structure with the projected and boundary by node and
class</td>
</tr>
<tr class="even">
<td>permute_importance</td>
<td>Obtain the permuted importance variable measure</td>
</tr>
<tr class="odd">
<td>ppf_avg_imp</td>
<td>Computes a global importance measure for a PPforest object, average
importance measure for a pptree over all the trees.</td>
</tr>
<tr class="even">
<td>ppf_global_imp</td>
<td>Computes a global importance measure for a PPforest object</td>
</tr>
<tr class="odd">
<td>PPforest</td>
<td>Runs a Projection pursuit random forest</td>
</tr>
<tr class="even">
<td>PPtree_split</td>
<td>Projection pursuit classification tree with random variable
selection in each split</td>
</tr>
<tr class="odd">
<td>print.PPforest</td>
<td>Print PPforest object</td>
</tr>
<tr class="even">
<td>predict.PPforest</td>
<td>Predict class for the test set and calculate prediction error</td>
</tr>
<tr class="odd">
<td>ternary_str</td>
<td>Data structure with the projected and boundary by node and
class</td>
</tr>
</tbody>
</table>
<p>Also <code>PPforest</code> package includes some data set that were
used to test the predictive performance of our method. The data sets
included are: crab, fishcatch, glass, image, leukemia, lymphoma NCI60,
parkinson and wine.</p>
</div>
<div class="section level3">
<h3 id="example">Example<a class="anchor" aria-label="anchor" href="#example"></a>
</h3>
<p>Australian crab data set will be used as example. This data contains
measurements on rock crabs of the genus Leptograpsus. There are 200
observations from two species (blue and orange) and for each specie (50
in each one) there are 50 males and 50 females. Class variable has 4
classes with the combinations of specie and sex (BlueMale, BlueFemale,
OrangeMale and OrangeFemale). The data were collected on site at
Fremantle, Western Australia. For each specimen, five measurements were
made, using vernier calipers.</p>
<ol style="list-style-type: decimal">
<li>FL the size of the frontal lobe length, in mm</li>
<li>RW rear width, in mm</li>
<li>CL length of mid line of the carapace, in mm</li>
<li>CW maximum width of carapace, in mm</li>
<li>BD depth of the body; for females, measured after displacement of
the abdomen, in mm</li>
</ol>
<p>To visualize this data set we use a scatterplot matrix from the
package <code>GGally</code></p>
<p> </p>
<p> </p>
<div class="figure" style="text-align: center">
<img src="PPforest-vignette_files/figure-html/descri-1.png" alt="Scatter plot matrix of crab data " width="480"><p class="caption">
Scatter plot matrix of crab data
</p>
</div>
<p> </p>
<p> </p>
<p>In this figure we can see a strong, positive and linear association
between the different variables. Also look like the classes can be
separated by linear combinations.</p>
<p>The main function of the package is <code>PPforest</code> which
implements a projection pursuit random forest.</p>
<p><code>PPtree_split</code> this function implements a projection
pursuit classification tree with random variable selection in each
split, based on the original PPtree algorithm from
<code>PPtreeViz</code> R package <span class="citation">(E.-K. Lee
2018)</span>. This function returns a <code>PPtreeclass</code> object.
To use this function we need to specify a formula describing the model
to be fitted response~predictors (<code>form</code>), <code>data</code>
is a data frame with the complete data set. Also we need to specify the
method <code>PPmethod</code>, it is the index to use for projection
pursuit: ‘LDA’ or ‘PDA’, <code>size.p</code> is the proportion of
variables randomly sampled in each split. If size.p = 1 a classic
<code>PPtreeclass</code> object will be fitted using all the variables
in each node partition instead of a subset of them. <code>lambda</code>
penalty parameter in PDA index and is between 0 to 1 . he following
example fits a projection pursuit classification tree constructed using
0.6 of the variables (3 out of 5) in each node split. We selected
<code>LDA</code> method.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Tree.crab</span> <span class="op">&lt;-</span> <span class="fu">PPforest</span><span class="fu">::</span><span class="fu"><a href="../reference/PPtree_split.html">PPtree_split</a></span><span class="op">(</span><span class="st">"Type~."</span>, data <span class="op">=</span> <span class="va">crab</span>, PPmethod <span class="op">=</span> <span class="st">"LDA"</span>, size.p <span class="op">=</span> <span class="fl">0.6</span><span class="op">)</span></span>
<span> <span class="va">Tree.crab</span></span></code></pre></div>
<pre><code><span><span class="co">## ============================================================= </span></span>
<span><span class="co">## Projection Pursuit Classification Tree result </span></span>
<span><span class="co">## =============================================================</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 1) root</span></span>
<span><span class="co">##    2)  proj1*X &lt; cut1</span></span>
<span><span class="co">##       4)* proj2*X &lt; cut2  -&gt;  "BlueMale"</span></span>
<span><span class="co">##       5)* proj2*X &gt;= cut2  -&gt;  "BlueFemale"</span></span>
<span><span class="co">##    3)  proj1*X &gt;= cut1</span></span>
<span><span class="co">##       6)* proj3*X &lt; cut3  -&gt;  "OrangeMale"</span></span>
<span><span class="co">##       7)* proj3*X &gt;= cut3  -&gt;  "OrangeFemale"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Error rates of various cutoff values </span></span>
<span><span class="co">## -------------------------------------------------------------</span></span>
<span><span class="co">##            Rule1 Rule2 Rule3 Rule4 Rule5 Rule6 Rule7 Rule8</span></span>
<span><span class="co">## error.rate 0.065 0.065 0.065 0.065 0.075 0.075 0.075 0.075</span></span></code></pre>
<p><code>PPforest</code> function runs a projection pursuit random
forest. The arguments are <code>data</code> a data.frame with the data
information, <code>y</code> a character with the name of the class
variable. <code>size.tr</code> to specify the proportion of observations
using in the training. Using this function we have the option to split
the data in training and test using <code>size.tr</code> internally in
the <code>PPforest</code> function. <code>size.tr</code> is the
proportion of data used in the training and the test proportion will be
1- <code>size.tr</code>. The number of trees in the forest is specified
using the argument <code>m</code>. The argument <code>size.p</code> is
the sample proportion of the variables used in each node split,
<code>PPmethod</code> is the projection pursuit index to be optimized,
two options LDA and PDA are available. The algorithm can be parallelized
using <code>parallel</code> and <code>cores</code> arguments.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">pprf.crab</span> <span class="op">&lt;-</span> <span class="fu">PPforest</span><span class="fu">::</span><span class="fu"><a href="../reference/PPforest.html">PPforest</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">crab</span>, y <span class="op">=</span> <span class="st">"Type"</span>, std <span class="op">=</span> <span class="st">'min-max'</span>, size.tr <span class="op">=</span> <span class="fl">.7</span>, m <span class="op">=</span> <span class="fl">200</span>,</span>
<span>                                size.p <span class="op">=</span>  <span class="fl">.8</span>,  PPmethod <span class="op">=</span> <span class="st">'LDA'</span>,  parallel <span class="op">=</span> <span class="cn">TRUE</span>, cores <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="va">pprf.crab</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">## Call:</span></span>
<span><span class="co">##  PPforest::PPforest(data = crab, y = "Type", std = "min-max",      size.tr = 0.7, m = 200, PPmethod = "LDA", size.p = 0.8, parallel = TRUE,      cores = 2) </span></span>
<span><span class="co">##                Type of random forest: Classification</span></span>
<span><span class="co">##                      Number of trees: 200</span></span>
<span><span class="co">## No. of variables tried at each split: 4</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##         OOB estimate of  error rate: 5.71%</span></span>
<span><span class="co">## Confusion matrix:</span></span>
<span><span class="co">##              BlueFemale BlueMale OrangeFemale OrangeMale class.error</span></span>
<span><span class="co">## BlueFemale           33        2            0          0        0.06</span></span>
<span><span class="co">## BlueMale              4       31            0          0        0.11</span></span>
<span><span class="co">## OrangeFemale          0        0           33          2        0.06</span></span>
<span><span class="co">## OrangeMale            0        0            0         35        0.00</span></span></code></pre>
<p><code>PPforest</code> print a summary result from the model with the
confusion matrix information and the oob-error rate in a similar way
<code>randomForest</code> packages does.</p>
<p>This function returns the predicted values of the training data,
training error, test error and predicted test values. Also there is the
information about out of bag error for the forest and also for each tree
in the forest. Bootstrap samples, output of all the trees in the forest
from , proximity matrix and vote matrix, number of trees grown in the
forest, number of predictor variables selected to use for splitting at
each node. Confusion matrix of the prediction (based on OOb data), the
training data and test data and vote matrix are also returned.</p>
<p>The printed version of a <code>PPforest</code> object follows the
<code>randomForest</code> printed version to make them comparable. Based
on confusion matrix, we can observe that the biggest error is for
BlueMale class. Most of the wrong classified values are between
BlueFemale and BlueMale.</p>
<p>The output from a <code>PPforest</code> object contains a lot of
information as we can see in the next output.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/str.html" class="external-link">str</a></span><span class="op">(</span><span class="va">pprf.crab</span>, max.level <span class="op">=</span> <span class="fl">1</span> <span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## List of 28</span></span>
<span><span class="co">##  $ predicting.training: Factor w/ 4 levels "BlueFemale","BlueMale",..: 2 2 2 1 2 2 2 1 2 2 ...</span></span>
<span><span class="co">##  $ training.error     : num 0.05</span></span>
<span><span class="co">##  $ prediction.test    : Factor w/ 4 levels "BlueFemale","BlueMale",..: 2 2 2 1 2 1 2 2 2 2 ...</span></span>
<span><span class="co">##  $ error.test         : num 0.0833</span></span>
<span><span class="co">##  $ oob.error.forest   : num 0.0571</span></span>
<span><span class="co">##  $ oob.error.tree     : num [1:200, 1] 0.1346 0.06 0.08 0.08 0.0222 ...</span></span>
<span><span class="co">##  $ boot.samp          :List of 200</span></span>
<span><span class="co">##  $ output.trees       :List of 200</span></span>
<span><span class="co">##  $ proximity          : num [1:140, 1:140] 0 0.805 0.825 0.35 0.835 0.675 0.665 0.625 0.805 0.635 ...</span></span>
<span><span class="co">##  $ votes              : num [1:140, 1:4] 0.342 0.514 0.2 0.701 0.175 ...</span></span>
<span><span class="co">##   ..- attr(*, "dimnames")=List of 2</span></span>
<span><span class="co">##  $ prediction.oob     : Factor w/ 4 levels "BlueFemale","BlueMale",..: 2 1 2 1 2 2 2 1 2 2 ...</span></span>
<span><span class="co">##  $ n.tree             : num 200</span></span>
<span><span class="co">##  $ n.var              : int 4</span></span>
<span><span class="co">##  $ type               : chr "Classification"</span></span>
<span><span class="co">##  $ confusion          : num [1:4, 1:5] 33 4 0 0 2 31 0 0 0 0 ...</span></span>
<span><span class="co">##   ..- attr(*, "dimnames")=List of 2</span></span>
<span><span class="co">##  $ call               : language PPforest::PPforest(data = crab, y = "Type", std = "min-max", size.tr = 0.7,      m = 200, PPmethod = "LDA", size.| __truncated__</span></span>
<span><span class="co">##  $ train              :'data.frame': 140 obs. of  6 variables:</span></span>
<span><span class="co">##  $ test               :'data.frame': 60 obs. of  6 variables:</span></span>
<span><span class="co">##  $ vote.mat           : num [1:200, 1:140] 2 4 2 2 2 2 2 2 2 1 ...</span></span>
<span><span class="co">##   ..- attr(*, "dimnames")=List of 2</span></span>
<span><span class="co">##  $ vote.mat_cl        : chr [1:4] "BlueFemale" "BlueMale" "OrangeFemale" "OrangeMale"</span></span>
<span><span class="co">##  $ class.var          : chr "Type"</span></span>
<span><span class="co">##  $ oob.obs            : num [1:200, 1:140] 0 0 0 1 0 1 0 0 0 0 ...</span></span>
<span><span class="co">##  $ std                : chr "min-max"</span></span>
<span><span class="co">##  $ dataux             : tibble [140 × 5] (S3: tbl_df/tbl/data.frame)</span></span>
<span><span class="co">##  $ mincol             : Named num [1:5] 8.1 6.7 16.1 18.6 7</span></span>
<span><span class="co">##   ..- attr(*, "names")= chr [1:5] "FL" "RW" "CL" "CW" ...</span></span>
<span><span class="co">##  $ maxmincol          : Named num [1:5] 15 13.5 31.5 36 14.6</span></span>
<span><span class="co">##   ..- attr(*, "names")= chr [1:5] "FL" "RW" "CL" "CW" ...</span></span>
<span><span class="co">##  $ train_mean         : NULL</span></span>
<span><span class="co">##  $ train_sd           : NULL</span></span>
<span><span class="co">##  - attr(*, "class")= chr "PPforest"</span></span></code></pre>
<p>For example to get the predicted values for the test data we can use
the PPforest output:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">pprf.crab</span><span class="op">$</span><span class="va">prediction.test</span></span></code></pre></div>
<pre><code><span><span class="co">##  [1] BlueMale     BlueMale     BlueMale     BlueFemale   BlueMale    </span></span>
<span><span class="co">##  [6] BlueFemale   BlueMale     BlueMale     BlueMale     BlueMale    </span></span>
<span><span class="co">## [11] BlueMale     BlueMale     BlueMale     BlueMale     BlueMale    </span></span>
<span><span class="co">## [16] BlueMale     BlueFemale   BlueFemale   BlueFemale   BlueFemale  </span></span>
<span><span class="co">## [21] BlueFemale   BlueFemale   BlueFemale   BlueFemale   BlueFemale  </span></span>
<span><span class="co">## [26] BlueFemale   BlueFemale   BlueFemale   BlueFemale   BlueFemale  </span></span>
<span><span class="co">## [31] OrangeMale   OrangeMale   OrangeMale   OrangeMale   OrangeMale  </span></span>
<span><span class="co">## [36] OrangeMale   OrangeMale   OrangeMale   OrangeMale   OrangeMale  </span></span>
<span><span class="co">## [41] OrangeMale   OrangeMale   OrangeMale   OrangeMale   OrangeMale  </span></span>
<span><span class="co">## [46] OrangeMale   OrangeMale   OrangeFemale OrangeFemale OrangeFemale</span></span>
<span><span class="co">## [51] OrangeFemale OrangeFemale OrangeFemale OrangeFemale OrangeFemale</span></span>
<span><span class="co">## [56] OrangeFemale OrangeFemale OrangeFemale OrangeFemale OrangeFemale</span></span>
<span><span class="co">## Levels: BlueFemale BlueMale OrangeFemale OrangeMale</span></span></code></pre>
<p>If new data are available you can use the function
<code>trees_pred</code> to get the predicted classes by PPforest
object.</p>
<p><code>predict(object = pprf.crab,  newdata)</code></p>
<p>The PPforest algorithm calculates variable importance in two ways:
(1) permuted importance using accuracy, and (2) importance based on
projection coefficients on standardized variables.</p>
<p>The permuted variable importance is comparable with the measure
defined in the classical random forest algorithm. It is computed using
the out of bag (oob) sample for the tree
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mspace width="0.278em"></mspace><mspace width="0.278em"></mspace><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>B</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">k\;\;(B^{(k)})</annotation></semantics></math>
for each
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>X</mi><mi>j</mi></msub><annotation encoding="application/x-tex">X_j</annotation></semantics></math>
predictor variable. Then the permuted importance of the variable
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>X</mi><mi>j</mi></msub><annotation encoding="application/x-tex">X_j</annotation></semantics></math>
in the tree
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
can be defined as:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mi>M</mi><msup><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>X</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><msup><mi>B</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow></munder><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><msubsup><mover><mi>y</mi><mo accent="true">̂</mo></mover><mi>i</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><msubsup><mover><mi>y</mi><mo accent="true">̂</mo></mover><mrow><mi>i</mi><mo>,</mo><msub><mi>P</mi><mi>j</mi></msub></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mo stretchy="true" form="prefix">|</mo><msup><mi>B</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">|</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">
IMP^{(k)}(X_j) = \frac{\sum_{i \in B^{(k)} } I(y_i=\hat y_i^{(k)})-I(y_i=\hat y_{i,P_j}^{(k)})}{|B^{(k)}|}
</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mover><mi>y</mi><mo accent="true">̂</mo></mover><mi>i</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><annotation encoding="application/x-tex">\hat y_i^{(k)}</annotation></semantics></math>
is the predicted class for the observation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
in the tree
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>y</mi><mrow><mi>i</mi><mo>,</mo><msub><mi>P</mi><mi>j</mi></msub></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><annotation encoding="application/x-tex">y_{i,P_j}^{(k)}</annotation></semantics></math>
is the predicted class for the observation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
in the tree
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
after permuting the values for variable
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>X</mi><mi>j</mi></msub><annotation encoding="application/x-tex">X_j</annotation></semantics></math>.
The global permuted importance measure is the average importance over
all the trees in the forest. This measure is based on comparing the
accuracy of classifying out-of-bag observations, using the true class
with permuted (nonsense) class. To compute this measure you should use
permute_importance function.  </p>
<p> </p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">impo1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/permute_importance.html">permute_importance</a></span><span class="op">(</span><span class="va">pprf.crab</span><span class="op">)</span></span>
<span><span class="va">impo1</span></span></code></pre></div>
<pre><code><span><span class="co">##   nm    imp   sd.imp      imp2   sd.imp2 imp2.std  imp.std</span></span>
<span><span class="co">## 1 BD 17.285 7.678448 0.3403903 0.1465104 2.323318 2.251106</span></span>
<span><span class="co">## 2 RW 18.295 6.377408 0.3615318 0.1231153 2.936530 2.868720</span></span>
<span><span class="co">## 3 CW 21.465 6.615710 0.4245943 0.1291092 3.288646 3.244550</span></span>
<span><span class="co">## 4 CL 21.790 7.267737 0.4300271 0.1394407 3.083942 2.998182</span></span>
<span><span class="co">## 5 FL 22.215 7.564714 0.4397407 0.1462055 3.007689 2.936661</span></span></code></pre>
<p> </p>
<p> </p>
<div class="figure" style="text-align: center">
<img src="PPforest-vignette_files/figure-html/figimp1-1.png" alt="Permuted importance variable" width="700"><p class="caption">
Permuted importance variable
</p>
</div>
<p> </p>
<p> </p>
<p>This function returns a data frame with permuted importance measures,
imp is the permuted importance measure defined in Brieman paper, imp2 is
the permuted importance measure defined in randomForest package, the
standard deviation (sd.im and sd.imp2) for each measure is computed and
the also the standardized measure.</p>
<p>For the second importance measure, the coefficients of each
projection are examined. The magnitude of these values indicates
importance, if the variables have been standardized. The variable
importance for a single tree is computed by a weighted sum of the
absolute values of the coefficients across nodes. The weights takes the
number of classes in each node into account <span class="citation">(Y.
D. Lee et al. 2013)</span>. Then the importance of the variable
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>X</mi><mi>j</mi></msub><annotation encoding="application/x-tex">X_j</annotation></semantics></math>
in the PPtree
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
can be defined as:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mi>M</mi><msubsup><mi>P</mi><mrow><mi>p</mi><mi>p</mi><mi>t</mi><mi>r</mi><mi>e</mi><mi>e</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>X</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munderover><mo>∑</mo><mrow><mi>n</mi><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><mi>n</mi></mrow></munderover><mfrac><mrow><mo stretchy="true" form="prefix">|</mo><msubsup><mi>α</mi><mrow><mi>n</mi><mi>d</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mi>c</mi><msub><mi>l</mi><mrow><mi>n</mi><mi>d</mi></mrow></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">
  IMP_{pptree}^{(k)}(X_j)=\sum_{nd = 1}^{nn}\frac{|\alpha_{nd}^{(k)}|}{cl_{nd} }
</annotation></semantics></math></p>
<p>Where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>α</mi><mrow><mi>n</mi><mi>d</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><annotation encoding="application/x-tex">\alpha_{nd}^{(k)}</annotation></semantics></math>
is the projected coefficient for node
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">ns</annotation></semantics></math>
and variable
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">nn</annotation></semantics></math>
the total number of node partitions in the tree
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>.</p>
<p>The global variable importance in a PPforest then can be defined in
different ways. The most intuitive is the average variable importance
from each PPtree across all the trees in the forest.</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mi>M</mi><msub><mi>P</mi><mrow><mi>p</mi><mi>p</mi><mi>f</mi><mi>o</mi><mi>r</mi><mi>e</mi><mi>s</mi><mi>t</mi><mn>1</mn></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>X</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mi>I</mi><mi>M</mi><msubsup><mi>P</mi><mrow><mi>p</mi><mi>p</mi><mi>t</mi><mi>r</mi><mi>e</mi><mi>e</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>X</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mi>K</mi></mfrac></mrow><annotation encoding="application/x-tex">
IMP_{ppforest1}(X_j)=\frac{\sum_{k=1}^K IMP_{pptree}^{(k)}(X_j)}{K}
</annotation></semantics></math></p>
<p>Alternatively we have defined a global importance measure for the
forest as a weighted mean of the absolute value of the projection
coefficients across all nodes in every tree. The weights are based on
the projection pursuit indexes in each node
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><msub><mi>x</mi><mrow><mi>n</mi><mi>d</mi></mrow></msub></mrow><annotation encoding="application/x-tex">Ix_{nd}</annotation></semantics></math>),
and 1-(OOB-error of each
tree)(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mi>c</mi><msub><mi>c</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">acc_k</annotation></semantics></math>).</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mi>M</mi><msub><mi>P</mi><mrow><mi>p</mi><mi>p</mi><mi>f</mi><mi>o</mi><mi>r</mi><mi>e</mi><mi>s</mi><mi>t</mi><mn>2</mn></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>X</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mi>a</mi><mi>c</mi><msub><mi>c</mi><mi>k</mi></msub><munderover><mo>∑</mo><mrow><mi>n</mi><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><mi>n</mi></mrow></munderover><mfrac><mrow><mi>I</mi><msub><mi>x</mi><mrow><mi>n</mi><mi>d</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">|</mo><msubsup><mi>α</mi><mrow><mi>n</mi><mi>d</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo stretchy="true" form="postfix">|</mo></mrow></mrow><mrow><mi>n</mi><mi>n</mi></mrow></mfrac></mrow><mi>K</mi></mfrac></mrow><annotation encoding="application/x-tex">IMP_{ppforest2}(X_j)=\frac{\sum_{k=1}^K acc_k \sum_{nd = 1}^{nn}\frac{Ix_{nd}|\alpha_{nd}^{(k)}|}{nn }}{K}
</annotation></semantics></math>  </p>
<p> </p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">impo2</span> <span class="op">&lt;-</span>  <span class="fu"><a href="../reference/ppf_avg_imp.html">ppf_avg_imp</a></span><span class="op">(</span><span class="va">pprf.crab</span>, <span class="st">"Type"</span><span class="op">)</span></span>
<span><span class="va">impo2</span></span></code></pre></div>
<pre><code><span><span class="co">## <span style="color: #949494;"># A tibble: 5 × 2</span></span></span>
<span><span class="co">##   variable  mean</span></span>
<span><span class="co">##   <span style="color: #949494; font-style: italic;">&lt;fct&gt;</span>    <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">## <span style="color: #BCBCBC;">1</span> CL       0.573</span></span>
<span><span class="co">## <span style="color: #BCBCBC;">2</span> CW       0.532</span></span>
<span><span class="co">## <span style="color: #BCBCBC;">3</span> RW       0.431</span></span>
<span><span class="co">## <span style="color: #BCBCBC;">4</span> FL       0.294</span></span>
<span><span class="co">## <span style="color: #BCBCBC;">5</span> BD       0.185</span></span></code></pre>
<p> </p>
<p> </p>
<div class="figure" style="text-align: center">
<img src="PPforest-vignette_files/figure-html/figimp2-1.png" alt="Average importance variable" width="700"><p class="caption">
Average importance variable
</p>
</div>
<p> </p>
<p> </p>
<p>Finally you can get the last importance measure we have proposed for
the PPforest using `ppf_global_imp’ function.</p>
<p> </p>
<p> </p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">impo3</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/ppf_global_imp.html">ppf_global_imp</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">crab</span>, y <span class="op">=</span> <span class="st">"Type"</span>, <span class="va">pprf.crab</span><span class="op">)</span></span>
<span><span class="va">impo3</span></span></code></pre></div>
<pre><code><span><span class="co">## <span style="color: #949494;"># A tibble: 5 × 2</span></span></span>
<span><span class="co">##   variable  mean</span></span>
<span><span class="co">##   <span style="color: #949494; font-style: italic;">&lt;fct&gt;</span>    <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">## <span style="color: #BCBCBC;">1</span> CW       0.421</span></span>
<span><span class="co">## <span style="color: #BCBCBC;">2</span> CL       0.381</span></span>
<span><span class="co">## <span style="color: #BCBCBC;">3</span> RW       0.273</span></span>
<span><span class="co">## <span style="color: #BCBCBC;">4</span> FL       0.225</span></span>
<span><span class="co">## <span style="color: #BCBCBC;">5</span> BD       0.150</span></span></code></pre>
<p> </p>
<p> </p>
<div class="figure" style="text-align: center">
<img src="PPforest-vignette_files/figure-html/figimp3-1.png" alt="Global importance variable" width="700"><p class="caption">
Global importance variable
</p>
</div>
<p>Using the information available in the PPforest object, some
visualization can be done. I will include some useful examples to
visualize the data and some of the most important diagnostics in a
forest structure.</p>
<p>To describe the data structure a parallel plot can be done, the data
were standardized and the color represents the class variable.</p>
<p> </p>
<p> </p>
<div class="figure" style="text-align: center">
<img src="PPforest-vignette_files/figure-html/parallel-1.png" alt="Parallel coordinate plot of crab data" width="672"><p class="caption">
Parallel coordinate plot of crab data
</p>
</div>
<p> </p>
<p> </p>
<p><code>ternary_str</code> is an auxiliary functions in
<code>PPforest</code> to get the data structure needed to do a ternary
plot or a generalized ternary plot if more than 3 classes are available.
Because the PPforest is composed of many tree fits on subsets of the
data, a lot of statistics can be calculated to analyze as a separate
data set, and better understand how the model is working. Some of the
diagnostics of interest are: variable importance, OOB error rate, vote
matrix and proximity matrix.</p>
<p>With a decision tree we can compute for every pair of observations
the proximity matrix. This is a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mi>x</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">nxn</annotation></semantics></math>
matrix where if two cases
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>k</mi><mi>i</mi></msub><annotation encoding="application/x-tex">k_i</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>k</mi><mi>j</mi></msub><annotation encoding="application/x-tex">k_j</annotation></semantics></math>
are in the same terminal node increase their proximity by one, at the
end normalize the proximities by dividing by the number of trees. To
visualize the proximity matrix we use a scatter plot with information
from multidimensional scaling method. In this plot color indicates the
true species and sex. For this data two dimensions are enough to see the
four groups separated quite well. Some crabs are clearly more similar to
a different group, though, especially in examining the sex
differences.</p>
<p> </p>
<p> </p>
<div class="figure" style="text-align: center">
<img src="PPforest-vignette_files/figure-html/mds-1.png" alt="Multidimensional scaling plot to examine similarities between cases" width="480"><p class="caption">
Multidimensional scaling plot to examine similarities between cases
</p>
</div>
<p> </p>
<p> </p>
<p>The vote matrix
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">n \times p</annotation></semantics></math>)
contains the proportion of times each observation was classified to each
class, whole oob. Two possible approaches to visualize the vote matrix
information are shown, with a side-by-side jittered dot plot or with
ternary plots. A side-by-side jittered dotplot is used for the display,
where class is displayed on one axis and proportion is displayed on the
other. For each dotplot, the ideal arrangement is that points of
observations in that class have values bigger than 0.5, and all other
observations have less. This data is close to the ideal but not perfect,
e.g. there are a few blue male crabs (orange) that are frequently
predicted to be blue females (green), and a few blue female crabs
predicted to be another class.</p>
<p> </p>
<p> </p>
<div class="figure" style="text-align: center">
<img src="PPforest-vignette_files/figure-html/side-1.png" alt="Vote matrix representation by a jittered side-by-side dotplot. Each dotplot shows the proportion of times the case was predicted into the group, with 1 indicating that the case was always predicted to the group and 0 being never." width="480"><p class="caption">
Vote matrix representation by a jittered side-by-side dotplot. Each
dotplot shows the proportion of times the case was predicted into the
group, with 1 indicating that the case was always predicted to the group
and 0 being never.
</p>
</div>
<p> </p>
<p> </p>
<p>A ternary plot is a triangular diagram that shows the proportion of
three variables that sum to a constant and is done using barycentric
coordinates. Compositional data lies in a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(p-1)</annotation></semantics></math>-D
simplex in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-space.
One advantage of ternary plot is that are good to visualize
compositional data and the proportion of three variables in a two
dimensional space can be shown. When we have tree classes a ternary plot
are well defined. With more than tree classes the ternary plot idea need
to be generalized.@sutherland2000orca suggest the best approach to
visualize compositional data will be to project the data into the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo></mrow><annotation encoding="application/x-tex">(p-1)-</annotation></semantics></math>D
space (ternary diagram in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>−</mo><mi>D</mi></mrow><annotation encoding="application/x-tex">2-D</annotation></semantics></math>)
This will be the approach used to visualize the vote matrix
information.</p>
<p>A ternary plot is a triangular diagram used to display compositional
data with three components. More generally, compositional data can have
any number of components, say
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>,
and hence is contrained to a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(p-1)</annotation></semantics></math>-D
simplex in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-space.
The vote matrix is an example of compositional data, with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math>
components.  </p>
<p> </p>
<div class="figure" style="text-align: center">
<img src="PPforest-vignette_files/figure-html/ternary-1.png" alt="Generalized ternary plot representation of the vote matrix for four classes. The tetrahedron is shown pairwise. Each point corresponds to one observation and color is the true class." width="672"><p class="caption">
Generalized ternary plot representation of the vote matrix for four
classes. The tetrahedron is shown pairwise. Each point corresponds to
one observation and color is the true class.
</p>
</div>
<p> </p>
<p> </p>
<p>To see a complete description about how to visualize a PPforest
object read Interactive Graphics for Visually Diagnosing Forest
Classifiers in R <span class="citation">(da Silva, Cook, and Lee
2025)</span>.</p>
</div>
</div>
<div class="section level2 unnumbered">
<h2 class="unnumbered" id="references">REFERENCES<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-da2021projection" class="csl-entry">
da Silva, Natalia, Dianne Cook, and Eun-Kyung Lee. 2021. <span>“A
Projection Pursuit Forest Algorithm for Supervised
Classification.”</span> <em>Journal of Computational and Graphical
Statistics</em> 30 (4): 1168–80.
</div>
<div id="ref-da2025interactive" class="csl-entry">
———. 2025. <span>“Interactive Graphics for Visually Diagnosing Forest
Classifiers in r.”</span> <em>Computational Statistics</em> 40 (6):
3105–25.
</div>
<div id="ref-PPtreeVizpkg" class="csl-entry">
Lee, Eun-Kyung. 2018. <span>“<span>PPtreeViz</span>: An <span>R</span>
Package for Visualizing Projection Pursuit Classification Trees.”</span>
<em>Journal of Statistical Software</em> 83 (8): 1–30. <a href="https://doi.org/10.18637/jss.v083.i08" class="external-link">https://doi.org/10.18637/jss.v083.i08</a>.
</div>
<div id="ref-lee2013pptree" class="csl-entry">
Lee, Yoon Dong, Dianne Cook, Ji-won Park, Eun-Kyung Lee, et al. 2013.
<span>“PPtree: Projection Pursuit Classification Tree.”</span>
<em>Electronic Journal of Statistics</em> 7: 1369–86.
</div>
<div id="ref-devtools" class="csl-entry">
Wickham, Hadley, and Winston Chang. 2015. <em>Devtools: Tools to Make
Developing r Packages Easier</em>. <a href="https://CRAN.R-project.org/package=devtools" class="external-link">https://CRAN.R-project.org/package=devtools</a>.
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Natalia da Silva, Dianne Cook, Eun-Kyung Lee.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer>
</div>





  </body>
</html>
