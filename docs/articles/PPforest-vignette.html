<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="PPforest">
<title>Projection pursuit classification random forest • PPforest</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Projection pursuit classification random forest">
<meta property="og:description" content="PPforest">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-dark navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">PPforest</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.3</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/PPforest-vignette.html">Projection pursuit classification random forest</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="nav-link" href="https://github.com/natydasilva/PPforest/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Projection pursuit classification random forest</h1>
                        <h4 data-toc-skip class="author">N. da Silva, D.
Cook &amp; E.K Lee</h4>
            
            <h4 data-toc-skip class="date">2023-10-28</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/natydasilva/PPforest/blob/HEAD/vignettes/PPforest-vignette.Rmd"><code>vignettes/PPforest-vignette.Rmd</code></a></small>
      <div class="d-none name"><code>PPforest-vignette.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>The <code>PPforest</code> package (projection pursuit random forest)
contains functions to run a projection pursuit random forest for
classification problems. This method utilize combinations of variables
in each tree construction. In a random forest each split is based on a
single variable, chosen from a subset of predictors. In the
<code>PPforest</code>, each split is based on a linear combination of
randomly chosen variables. The linear combination is computed by
optimizing a projection pursuit index, to get a projection of the
variables that best separates the classes. The <code>PPforest</code>
uses the <code>PPtree</code> algorithm, which fits a single tree to the
data. Utilizing linear combinations of variables to separate classes
takes the correlation between variables into account, and can outperform
the basic forest when separations between groups occurs on combinations
of variables. Two projection pursuit indexes, LDA and PDA, are used for
<code>PPforest</code>.</p>
<p>To improve the speed performance <code>PPforest</code> package,
<code>PPtree</code> algorithm was translated to Rcpp.
<code>PPforest</code> package utilizes a number of R packages some of
them included in “suggests” not to load them all at package
start-up.</p>
<p>You can install the package from CRAN:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">install.package</span><span class="op">(</span><span class="va">PPforest</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/natydasilva/PPforest">PPforest</a></span><span class="op">)</span></span></code></pre></div>
<p>Or the development version of <code>PPforest</code> can be installed
from github using:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://devtools.r-lib.org/" class="external-link">devtools</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://remotes.r-lib.org/reference/install_github.html" class="external-link">install_github</a></span><span class="op">(</span><span class="st">"natydasilva/PPforest"</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/natydasilva/PPforest">PPforest</a></span><span class="op">)</span></span></code></pre></div>
<p>##Projection pursuit classification forest</p>
<p>In <code>PPforest</code>, projection pursuit classification trees are
used as the individual model to be combined in the forest. The original
algorithm is in <code>PPtreeViz</code> package, we translate the
original tree algorithm into <code>Rcpp</code> to improve the speed
performance to run the forest.</p>
<p>One important characteristic of PPtree is that treats the data always
as a two-class system, when the classes are more than two the algorithm
uses a two step projection pursuits optimization in every node split.
Let <span class="math inline">\((X_i,y_i)\)</span> the data set, <span class="math inline">\(X_i\)</span> is a p-dimensional vector of
explanatory variables and <span class="math inline">\(y_i\in {1,2,\ldots
G}\)</span> represents class information with <span class="math inline">\(i=1,\ldots n\)</span>.</p>
<p>In the first step optimize a projection pursuit index to find an
optimal one-dimension projection <span class="math inline">\(\alpha^*\)</span> for separating all classes in
the current data. With the projected data redefine the problem in a two
class problem by comparing means, and assign a new label <span class="math inline">\(G1\)</span> or <span class="math inline">\(G2\)</span> to each observation, a new variable
<span class="math inline">\(y_i^*\)</span> is created. The new groups
<span class="math inline">\(G1\)</span> and <span class="math inline">\(G2\)</span> can contain more than one original
classes. Next step is to find an optimal one-dimensional projection
<span class="math inline">\(\alpha\)</span>, using <span class="math inline">\((X_i,y_i^*)\)</span> to separate the two class
problem <span class="math inline">\(G1\)</span> and <span class="math inline">\(G2\)</span>. The best separation of <span class="math inline">\(G1\)</span> and <span class="math inline">\(G2\)</span> is determine in this step and the
decision rule is defined for the current node, if <span class="math inline">\(\sum_{i=1}^p \alpha_i M1&lt; c\)</span> then
assign <span class="math inline">\(G1\)</span> to the left node else
assign <span class="math inline">\(G2\)</span> to the right node, where
<span class="math inline">\(M1\)</span> is the mean of <span class="math inline">\(G1\)</span>. For each groups we can repeat all the
previous steps until <span class="math inline">\(G1\)</span> and <span class="math inline">\(G2\)</span> have only one class from the original
classes. Base on this process to grow the tree, the depth of PPtree is
at most the number of classes because one class is assigned only to one
final node.</p>
<p>Trees from <code>PPtree</code> algorithm are simple, they use the
association between variables to find separation. If a linear boundary
exists, <code>PPtree</code> produces a tree without
misclassification.</p>
<p>Projection pursuit random forest algorithm description</p>
<ol style="list-style-type: decimal">
<li><p>Let N the number of cases in the training set <span class="math inline">\(\Theta=(X,Y)\)</span>, <span class="math inline">\(B\)</span> bootstrap samples from the training set
are taking (samples of size N with replacement).</p></li>
<li><p>For each bootstrap sample a \verb PPtree is grown to the largest
extent possible <span class="math inline">\(h(x, {\Theta_k})\)</span>.
No pruning. This tree is grown using step 3 modification.</p></li>
<li><p>Let M the number of input variables, a number of <span class="math inline">\(m&lt;&lt;M\)</span> variables are selected at
random at each node and the best split based on a linear combination of
these randomly chosen variables. The linear combination is computed by
optimizing a projection pursuit index, to get a projection of the
variables that best separates the classes.</p></li>
<li><p>Predict the classes of each case not included in the bootstrap
sample and compute oob error.</p></li>
<li><p>Based on majority vote predict the class for new data.</p></li>
</ol>
<p>###Overview PPforest package</p>
<p><code>PPforest</code> package implements a classification random
forest using projection pursuit classification trees. The following
table present all the functions in <code>PPforest</code> package.</p>
<table class="table">
<colgroup>
<col width="21%">
<col width="78%">
</colgroup>
<thead><tr class="header">
<th>Function</th>
<th>Description</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>baggtree</td>
<td>For each bootstrap sample grow a projection pursuit tree (PPtree
object).</td>
</tr>
<tr class="even">
<td>node_data</td>
<td>Data structure with the projected and boundary by node and
class</td>
</tr>
<tr class="odd">
<td>permute_importance</td>
<td>Obtain the permuted importance variable measure</td>
</tr>
<tr class="even">
<td>ppf_avg_imp</td>
<td>Computes a global importance measure for a PPforest object, average
importance measure for a pptree over all the trees.</td>
</tr>
<tr class="odd">
<td>PPclassify</td>
<td>Predict class for the test set and calculate prediction error after
finding the PPtree structure</td>
</tr>
<tr class="even">
<td>ppf_global_imp</td>
<td>Computes a global importance measure for a PPforest object</td>
</tr>
<tr class="odd">
<td>PPforest</td>
<td>Runs a Projection pursuit random forest</td>
</tr>
<tr class="even">
<td>PPtree_split</td>
<td>Projection pursuit classification tree with random variable
selection in each split</td>
</tr>
<tr class="odd">
<td>print.PPforest</td>
<td>Print PPforest object</td>
</tr>
<tr class="even">
<td>predict.PPforest</td>
<td>Predict class for the test set and calculate prediction error</td>
</tr>
<tr class="odd">
<td>ternary_str</td>
<td>Data structure with the projected and boundary by node and
class</td>
</tr>
<tr class="even">
<td>tree_pred</td>
<td>Obtain predicted class for new data from baggtree function or
PPforest</td>
</tr>
</tbody>
</table>
<p>Also <code>PPforest</code> package includes some data set that were
used to test the predictive performance of our method. The data sets
included are: crab, fishcatch, glass, image, leukemia, lymphoma NCI60,
parkinson and wine.</p>
<div class="section level3">
<h3 id="example">Example<a class="anchor" aria-label="anchor" href="#example"></a>
</h3>
<p>Australian crab data set will be used as example. This data contains
measurements on rock crabs of the genus Leptograpsus. There are 200
observations from two species (blue and orange) and for each specie (50
in each one) there are 50 males and 50 females. Class variable has 4
classes with the combinations of specie and sex (BlueMale, BlueFemale,
OrangeMale and OrangeFemale). The data were collected on site at
Fremantle, Western Australia. For each specimen, five measurements were
made, using vernier calipers.</p>
<ol style="list-style-type: decimal">
<li>FL the size of the frontal lobe length, in mm</li>
<li>RW rear width, in mm</li>
<li>CL length of mid line of the carapace, in mm</li>
<li>CW maximum width of carapace, in mm</li>
<li>BD depth of the body; for females, measured after displacement of
the abdomen, in mm</li>
</ol>
<p>To visualize this data set we use a scatterplot matrix from the
package <code>GGally</code></p>
<p> </p>
<p> </p>
<div class="figure" style="text-align: center">
<img src="PPforest-vignette_files/figure-html/descri-1.png" alt="Scatter plot matrix of crab data " width="480"><p class="caption">
Scatter plot matrix of crab data
</p>
</div>
<p> </p>
<p> </p>
<p>In this figure we can see a strong, positive and linear association
between the different variables. Also look like the classes can be
separated by linear combinations.</p>
<p>The main function of the package is <code>PPforest</code> which
implements a projection pursuit random forest.</p>
<p><code>PPtree_split</code> this function implements a projection
pursuit classification tree with random variable selection in each
split, based on the original PPtreeViz algorithm. This function returns
a <code>PPtreeclass</code> object. To use this function we need to
specify a formula describing the model to be fitted response~predictors
(<code>form</code>), <code>data</code> is a data frame with the complete
data set. Also we need to specify the method <code>PPmethod</code>, it
is the index to use for projection pursuit: ‘LDA’ or ‘PDA’,
<code>size.p</code> is the proportion of variables randomly sampled in
each split. If size.p = 1 a classic <code>PPtreeclass</code> object will
be fitted using all the variables in each node partition instead of a
subset of them. <code>lambda</code> penalty parameter in PDA index and
is between 0 to 1 . he following example fits a projection pursuit
classification tree constructed using 0.6 of the variables (3 out of 5)
in each node split. We selected <code>LDA</code> method.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Tree.crab</span> <span class="op">&lt;-</span> <span class="fu">PPforest</span><span class="fu">::</span><span class="fu"><a href="../reference/PPtree_split.html">PPtree_split</a></span><span class="op">(</span><span class="st">"Type~."</span>, data <span class="op">=</span> <span class="va">crab</span>, PPmethod <span class="op">=</span> <span class="st">"LDA"</span>, size.p <span class="op">=</span> <span class="fl">0.6</span><span class="op">)</span></span>
<span> <span class="va">Tree.crab</span></span></code></pre></div>
<pre><code><span><span class="co">## ============================================================= </span></span>
<span><span class="co">## Projection Pursuit Classification Tree result </span></span>
<span><span class="co">## =============================================================</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 1) root</span></span>
<span><span class="co">##    2)  proj1*X &lt; cut1</span></span>
<span><span class="co">##       4)* proj2*X &lt; cut2  -&gt;  "2"</span></span>
<span><span class="co">##       5)* proj2*X &gt;= cut2  -&gt;  "1"</span></span>
<span><span class="co">##    3)  proj1*X &gt;= cut1</span></span>
<span><span class="co">##       6)* proj3*X &lt; cut3  -&gt;  "4"</span></span>
<span><span class="co">##       7)* proj3*X &gt;= cut3  -&gt;  "3"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Error rates of various cutoff values </span></span>
<span><span class="co">## -------------------------------------------------------------</span></span>
<span><span class="co">##            Rule1 Rule2 Rule3 Rule4 Rule5 Rule6 Rule7 Rule8</span></span>
<span><span class="co">## error.rate 0.065 0.065 0.065 0.065 0.075 0.075 0.075 0.075</span></span></code></pre>
<p><code>PPforest</code> function runs a projection pursuit random
forest. The arguments are a data frame with the data information, class
with the name of the class variable argument. size.tr to specify the
proportion of observations using in the training. Using this function we
have the option to split the data in training and test using size.tr
directly. <code>size.tr</code> is the proportion of data used in the
training and the test proportion will be 1- <code>size.tr</code>. The
number of trees in the forest is specified using the argument
<code>m</code>. The argument size.p is the sample proportion of the
variables used in each node split, <code>PPmethod</code> is the
projection pursuit index to be optimized, two options LDA and PDA are
available.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">pprf.crab</span> <span class="op">&lt;-</span> <span class="fu">PPforest</span><span class="fu">::</span><span class="fu"><a href="../reference/PPforest.html">PPforest</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">crab</span>, class <span class="op">=</span> <span class="st">"Type"</span>, size.tr <span class="op">=</span> <span class="fl">.8</span>, m <span class="op">=</span> <span class="fl">200</span>,</span>
<span>                                size.p <span class="op">=</span>  <span class="fl">.5</span>,  PPmethod <span class="op">=</span> <span class="st">'LDA'</span>,  parallel <span class="op">=</span><span class="cn">FALSE</span>, cores <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="va">pprf.crab</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">## Call:</span></span>
<span><span class="co">##  PPforest::PPforest(data = crab, class = "Type", size.tr = 0.8,      m = 200, PPmethod = "LDA", size.p = 0.5, parallel = FALSE,      cores = 2) </span></span>
<span><span class="co">##                Type of random forest: Classification</span></span>
<span><span class="co">##                      Number of trees: 200</span></span>
<span><span class="co">## No. of variables tried at each split: 2</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##         OOB estimate of  error rate: 4.37%</span></span>
<span><span class="co">## Confusion matrix:</span></span>
<span><span class="co">##              BlueFemale BlueMale OrangeFemale OrangeMale class.error</span></span>
<span><span class="co">## BlueFemale           38        2            0          0        0.05</span></span>
<span><span class="co">## BlueMale              3       37            0          0        0.07</span></span>
<span><span class="co">## OrangeFemale          0        0           39          1        0.03</span></span>
<span><span class="co">## OrangeMale            0        1            0         39        0.03</span></span></code></pre>
<p><code>PPforest</code> print a summary result from the model with the
confusion matrix information and the oob-error rate in a similar way
randomForest packages does.</p>
<p>This function returns the predicted values of the training data,
training error, test error and predicted test values. Also there is the
information about out of bag error for the forest and also for each tree
in the forest. Bootstrap samples, output of all the trees in the forest
from , proximity matrix and vote matrix, number of trees grown in the
forest, number of predictor variables selected to use for splitting at
each node. Confusion matrix of the prediction (based on OOb data), the
training data and test data and vote matrix are also returned.</p>
<p>The printed version of a <code>PPforest</code> object follows the
<code>randomForest</code> printed version to make them comparable. Based
on confusion matrix, we can observe that the biggest error is for
BlueMale class. Most of the wrong classified values are between
BlueFemale and BlueMale.</p>
<p>The output from a <code>PPforest</code> object contains a lot of
information as we can see in the next output.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/str.html" class="external-link">str</a></span><span class="op">(</span><span class="va">pprf.crab</span>, max.level <span class="op">=</span> <span class="fl">1</span> <span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## List of 21</span></span>
<span><span class="co">##  $ predicting.training: Factor w/ 4 levels "BlueFemale","BlueMale",..: 2 2 2 2 1 2 2 2 1 2 ...</span></span>
<span><span class="co">##  $ training.error     : num 0.0375</span></span>
<span><span class="co">##  $ prediction.test    : Factor w/ 4 levels "BlueFemale","BlueMale",..: 2 2 1 2 2 2 2 2 2 2 ...</span></span>
<span><span class="co">##  $ error.test         : num 0.15</span></span>
<span><span class="co">##  $ oob.error.forest   : num 0.0437</span></span>
<span><span class="co">##  $ oob.error.tree     : num [1:200, 1] 0.1695 0.2679 0.0317 0.1207 0.3273 ...</span></span>
<span><span class="co">##  $ boot.samp          :List of 200</span></span>
<span><span class="co">##  $ output.trees       :List of 200</span></span>
<span><span class="co">##  $ proximity          : num [1:160, 1:160] 0 0.8 0.87 0.815 0.39 0.9 0.78 0.51 0.395 0.705 ...</span></span>
<span><span class="co">##  $ votes              : num [1:160, 1:4] 0.222 0.333 0.191 0.222 0.661 ...</span></span>
<span><span class="co">##   ..- attr(*, "dimnames")=List of 2</span></span>
<span><span class="co">##  $ prediction.oob     : Factor w/ 4 levels "BlueFemale","BlueMale",..: 2 2 2 2 1 2 2 2 1 2 ...</span></span>
<span><span class="co">##  $ n.tree             : num 200</span></span>
<span><span class="co">##  $ n.var              : num 2</span></span>
<span><span class="co">##  $ type               : chr "Classification"</span></span>
<span><span class="co">##  $ confusion          : num [1:4, 1:5] 38 3 0 0 2 37 0 1 0 0 ...</span></span>
<span><span class="co">##   ..- attr(*, "dimnames")=List of 2</span></span>
<span><span class="co">##  $ call               : language PPforest::PPforest(data = crab, class = "Type", size.tr = 0.8, m = 200,      PPmethod = "LDA", size.p = 0.5, para| __truncated__</span></span>
<span><span class="co">##  $ train              :'data.frame': 160 obs. of  6 variables:</span></span>
<span><span class="co">##  $ test               :'data.frame': 40 obs. of  5 variables:</span></span>
<span><span class="co">##  $ vote.mat           : num [1:200, 1:160] 1 1 2 2 1 1 2 4 4 2 ...</span></span>
<span><span class="co">##   ..- attr(*, "dimnames")=List of 2</span></span>
<span><span class="co">##  $ class.var          : chr "Type"</span></span>
<span><span class="co">##  $ oob.obs            : num [1:200, 1:160] 0 1 1 0 0 0 0 1 1 1 ...</span></span>
<span><span class="co">##  - attr(*, "class")= chr "PPforest"</span></span></code></pre>
<p>For example to get the predicted values for the test data we can use
the PPforest output:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">pprf.crab</span><span class="op">$</span><span class="va">prediction.test</span></span></code></pre></div>
<pre><code><span><span class="co">##  [1] BlueMale     BlueMale     BlueFemale   BlueMale     BlueMale    </span></span>
<span><span class="co">##  [6] BlueMale     BlueMale     BlueMale     BlueMale     BlueMale    </span></span>
<span><span class="co">## [11] BlueMale     BlueFemale   BlueFemale   BlueFemale   BlueFemale  </span></span>
<span><span class="co">## [16] BlueFemale   BlueFemale   BlueFemale   BlueFemale   BlueFemale  </span></span>
<span><span class="co">## [21] OrangeMale   OrangeMale   OrangeMale   OrangeMale   OrangeMale  </span></span>
<span><span class="co">## [26] OrangeMale   OrangeMale   OrangeMale   OrangeMale   OrangeMale  </span></span>
<span><span class="co">## [31] OrangeMale   OrangeMale   OrangeMale   OrangeMale   OrangeFemale</span></span>
<span><span class="co">## [36] OrangeFemale OrangeFemale OrangeFemale OrangeFemale OrangeFemale</span></span>
<span><span class="co">## Levels: BlueFemale BlueMale OrangeFemale OrangeMale</span></span></code></pre>
<p>If new data are available you can use the function
<code>trees_pred</code> to get the predicted classes by PPforest
object.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/trees_pred.html">trees_pred</a></span><span class="op">(</span><span class="va">pprf.crab</span>, xnew <span class="op">=</span> <span class="va">newdata</span>, parallel <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<p>The PPforest algorithm calculates variable importance in two ways:
(1) permuted importance using accuracy, and (2) importance based on
projection coefficients on standardized variables.</p>
<p>The permuted variable importance is comparable with the measure
defined in the classical random forest algorithm. It is computed using
the out of bag (oob) sample for the tree <span class="math inline">\(k\;\;(B^{(k)})\)</span> for each <span class="math inline">\(X_j\)</span> predictor variable. Then the permuted
importance of the variable <span class="math inline">\(X_j\)</span> in
the tree <span class="math inline">\(k\)</span> can be defined as:</p>
<p><span class="math display">\[
IMP^{(k)}(X_j) = \frac{\sum_{i \in B^{(k)} } I(y_i=\hat
y_i^{(k)})-I(y_i=\hat y_{i,P_j}^{(k)})}{|B^{(k)}|}
\]</span></p>
<p>where <span class="math inline">\(\hat y_i^{(k)}\)</span> is the
predicted class for the observation <span class="math inline">\(i\)</span> in the tree <span class="math inline">\(k\)</span> and <span class="math inline">\(y_{i,P_j}^{(k)}\)</span> is the predicted class
for the observation <span class="math inline">\(i\)</span> in the tree
<span class="math inline">\(k\)</span> after permuting the values for
variable <span class="math inline">\(X_j\)</span>. The global permuted
importance measure is the average importance over all the trees in the
forest. This measure is based on comparing the accuracy of classifying
out-of-bag observations, using the true class with permuted (nonsense)
class. To compute this measure you should use permute_importance
function.  </p>
<p> </p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">impo1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/permute_importance.html">permute_importance</a></span><span class="op">(</span><span class="va">pprf.crab</span><span class="op">)</span></span>
<span><span class="va">impo1</span></span></code></pre></div>
<pre><code><span><span class="co">##   nm    imp    sd.imp      imp2   sd.imp2 imp2.std  imp.std</span></span>
<span><span class="co">## 1 CW 17.975  9.342418 0.3104134 0.1596670 1.944129 1.924020</span></span>
<span><span class="co">## 2 BD 18.470 10.632703 0.3199451 0.1844948 1.734169 1.737094</span></span>
<span><span class="co">## 3 CL 20.505 10.005023 0.3559586 0.1728535 2.059308 2.049471</span></span>
<span><span class="co">## 4 FL 20.805 11.036576 0.3594990 0.1874475 1.917865 1.885096</span></span>
<span><span class="co">## 5 RW 21.195  9.461213 0.3668738 0.1613988 2.273088 2.240199</span></span></code></pre>
<p> </p>
<p> </p>
<div class="figure" style="text-align: center">
<img src="PPforest-vignette_files/figure-html/figimp1-1.png" alt="Permuted importance variable" width="700"><p class="caption">
Permuted importance variable
</p>
</div>
<p> </p>
<p> </p>
<p>This function returns a data frame with permuted importance measures,
imp is the permuted importance measure defined in Brieman paper, imp2 is
the permuted importance measure defined in randomForest package, the
standard deviation (sd.im and sd.imp2) for each measure is computed and
the also the standardized measure.</p>
<p>For the second importance measure, the coefficients of each
projection are examined. The magnitude of these values indicates
importance, if the variables have been standardized. The variable
importance for a single tree is computed by a weighted sum of the
absolute values of the coefficients across nodes. The weights takes the
number of classes in each node into account <span class="citation">(Lee
et al. 2013)</span>. Then the importance of the variable <span class="math inline">\(X_j\)</span> in the PPtree <span class="math inline">\(k\)</span> can be defined as:</p>
<p><span class="math display">\[
  IMP_{pptree}^{(k)}(X_j)=\sum_{nd =
1}^{nn}\frac{|\alpha_{nd}^{(k)}|}{cl_{nd} }
\]</span></p>
<p>Where <span class="math inline">\(\alpha_{nd}^{(k)}\)</span> is the
projected coefficient for node <span class="math inline">\(ns\)</span>
and variable <span class="math inline">\(k\)</span> and <span class="math inline">\(nn\)</span> the total number of node partitions in
the tree <span class="math inline">\(k\)</span>.</p>
<p>The global variable importance in a PPforest then can be defined in
different ways. The most intuitive is the average variable importance
from each PPtree across all the trees in the forest.</p>
<p><span class="math display">\[
IMP_{ppforest1}(X_j)=\frac{\sum_{k=1}^K IMP_{pptree}^{(k)}(X_j)}{K}
\]</span></p>
<p>Alternatively we have defined a global importance measure for the
forest as a weighted mean of the absolute value of the projection
coefficients across all nodes in every tree. The weights are based on
the projection pursuit indexes in each node (<span class="math inline">\(Ix_{nd}\)</span>), and 1-(OOB-error of each
tree)(<span class="math inline">\(acc_k\)</span>).</p>
<p><span class="math display">\[IMP_{ppforest2}(X_j)=\frac{\sum_{k=1}^K
acc_k \sum_{nd = 1}^{nn}\frac{Ix_{nd}|\alpha_{nd}^{(k)}|}{nn }}{K}
\]</span>  </p>
<p> </p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">impo2</span> <span class="op">&lt;-</span>  <span class="fu"><a href="../reference/ppf_avg_imp.html">ppf_avg_imp</a></span><span class="op">(</span><span class="va">pprf.crab</span>, <span class="st">"Type"</span><span class="op">)</span></span>
<span><span class="va">impo2</span></span></code></pre></div>
<pre><code><span><span class="co">## <span style="color: #949494;"># A tibble: 5 × 2</span></span></span>
<span><span class="co">##   variable  mean</span></span>
<span><span class="co">##   <span style="color: #949494; font-style: italic;">&lt;fct&gt;</span>    <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">## <span style="color: #BCBCBC;">1</span> CL       0.462</span></span>
<span><span class="co">## <span style="color: #BCBCBC;">2</span> CW       0.452</span></span>
<span><span class="co">## <span style="color: #BCBCBC;">3</span> RW       0.385</span></span>
<span><span class="co">## <span style="color: #BCBCBC;">4</span> BD       0.313</span></span>
<span><span class="co">## <span style="color: #BCBCBC;">5</span> FL       0.279</span></span></code></pre>
<p> </p>
<p> </p>
<div class="figure" style="text-align: center">
<img src="PPforest-vignette_files/figure-html/figimp2-1.png" alt="Average importance variable" width="700"><p class="caption">
Average importance variable
</p>
</div>
<p> </p>
<p> </p>
<p>Finally you can get the last importance measure we have proposed for
the PPforest using `ppf_global_imp’ function.</p>
<p> </p>
<p> </p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">impo3</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/ppf_global_imp.html">ppf_global_imp</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">crab</span>, class <span class="op">=</span> <span class="st">"Type"</span>, <span class="va">pprf.crab</span><span class="op">)</span></span>
<span><span class="va">impo3</span></span></code></pre></div>
<pre><code><span><span class="co">## <span style="color: #949494;"># A tibble: 5 × 2</span></span></span>
<span><span class="co">##   variable  mean</span></span>
<span><span class="co">##   <span style="color: #949494; font-style: italic;">&lt;fct&gt;</span>    <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">## <span style="color: #BCBCBC;">1</span> CW       0.419</span></span>
<span><span class="co">## <span style="color: #BCBCBC;">2</span> CL       0.384</span></span>
<span><span class="co">## <span style="color: #BCBCBC;">3</span> RW       0.335</span></span>
<span><span class="co">## <span style="color: #BCBCBC;">4</span> BD       0.281</span></span>
<span><span class="co">## <span style="color: #BCBCBC;">5</span> FL       0.236</span></span></code></pre>
<p> </p>
<p> </p>
<div class="figure" style="text-align: center">
<img src="PPforest-vignette_files/figure-html/figimp3-1.png" alt="Global importance variable" width="700"><p class="caption">
Global importance variable
</p>
</div>
<p>Using the information available in the PPforest object, some
visualization can be done. I will include some useful examples to
visualize the data and some of the most important diagnostics in a
forest structure.</p>
<p>To describe the data structure a parallel plot can be done, the data
were standardized and the color represents the class variable.</p>
<p> </p>
<p> </p>
<div class="figure" style="text-align: center">
<img src="PPforest-vignette_files/figure-html/parallel-1.png" alt="Parallel coordinate plot of crab data" width="672"><p class="caption">
Parallel coordinate plot of crab data
</p>
</div>
<p> </p>
<p> </p>
<p><code>ternary_str</code> is an auxiliary functions in
<code>PPforest</code> to get the data structure needed to do a ternary
plot or a generalized ternary plot if more than 3 classes are available.
Because the PPforest is composed of many tree fits on subsets of the
data, a lot of statistics can be calculated to analyze as a separate
data set, and better understand how the model is working. Some of the
diagnostics of interest are: variable importance, OOB error rate, vote
matrix and proximity matrix.</p>
<p>With a decision tree we can compute for every pair of observations
the proximity matrix. This is a <span class="math inline">\(nxn\)</span>
matrix where if two cases <span class="math inline">\(k_i\)</span> and
<span class="math inline">\(k_j\)</span> are in the same terminal node
increase their proximity by one, at the end normalize the proximities by
dividing by the number of trees. To visualize the proximity matrix we
use a scatter plot with information from multidimensional scaling
method. In this plot color indicates the true species and sex. For this
data two dimensions are enough to see the four groups separated quite
well. Some crabs are clearly more similar to a different group, though,
especially in examining the sex differences.</p>
<p> </p>
<p> </p>
<div class="figure" style="text-align: center">
<img src="PPforest-vignette_files/figure-html/mds-1.png" alt="Multidimensional scaling plot to examine similarities between cases" width="480"><p class="caption">
Multidimensional scaling plot to examine similarities between cases
</p>
</div>
<p> </p>
<p> </p>
<p>The vote matrix (<span class="math inline">\(n \times p\)</span>)
contains the proportion of times each observation was classified to each
class, whole oob. Two possible approaches to visualize the vote matrix
information are shown, with a side-by-side jittered dot plot or with
ternary plots. A side-by-side jittered dotplot is used for the display,
where class is displayed on one axis and proportion is displayed on the
other. For each dotplot, the ideal arrangement is that points of
observations in that class have values bigger than 0.5, and all other
observations have less. This data is close to the ideal but not perfect,
e.g. there are a few blue male crabs (orange) that are frequently
predicted to be blue females (green), and a few blue female crabs
predicted to be another class.</p>
<p> </p>
<p> </p>
<div class="figure" style="text-align: center">
<img src="PPforest-vignette_files/figure-html/side-1.png" alt="Vote matrix representation by a jittered side-by-side dotplot. Each dotplot shows the proportion of times the case was predicted into the group, with 1 indicating that the case was always predicted to the group and 0 being never." width="480"><p class="caption">
Vote matrix representation by a jittered side-by-side dotplot. Each
dotplot shows the proportion of times the case was predicted into the
group, with 1 indicating that the case was always predicted to the group
and 0 being never.
</p>
</div>
<p> </p>
<p> </p>
<p>A ternary plot is a triangular diagram that shows the proportion of
three variables that sum to a constant and is done using barycentric
coordinates. Compositional data lies in a <span class="math inline">\((p-1)\)</span>-D simplex in <span class="math inline">\(p\)</span>-space. One advantage of ternary plot is
that are good to visualize compositional data and the proportion of
three variables in a two dimensional space can be shown. When we have
tree classes a ternary plot are well defined. With more than tree
classes the ternary plot idea need to be generalized.@sutherland2000orca
suggest the best approach to visualize compositional data will be to
project the data into the <span class="math inline">\((p-1)-\)</span>D
space (ternary diagram in <span class="math inline">\(2-D\)</span>) This
will be the approach used to visualize the vote matrix information.</p>
<p>A ternary plot is a triangular diagram used to display compositional
data with three components. More generally, compositional data can have
any number of components, say <span class="math inline">\(p\)</span>,
and hence is contrained to a <span class="math inline">\((p-1)\)</span>-D simplex in <span class="math inline">\(p\)</span>-space. The vote matrix is an example of
compositional data, with <span class="math inline">\(G\)</span>
components.  </p>
<p> </p>
<div class="figure" style="text-align: center">
<img src="PPforest-vignette_files/figure-html/ternary-1.png" alt="Generalized ternary plot representation of the vote matrix for four classes. The tetrahedron is shown pairwise. Each point corresponds to one observation and color is the true class." width="672"><p class="caption">
Generalized ternary plot representation of the vote matrix for four
classes. The tetrahedron is shown pairwise. Each point corresponds to
one observation and color is the true class.
</p>
</div>
<p> </p>
<p> </p>
<p>To see a complete description about how to visualize a PPforest
object read Interactive Graphics for Visually Diagnosing Forest
Classifiers in R <span class="citation">(Silva, Cook, and Lee
2017)</span>.</p>
</div>
</div>
<div class="section level2 unnumbered">
<h2 class="unnumbered" id="references">REFERENCES<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-lee2013pptree" class="csl-entry">
Lee, Yoon Dong, Dianne Cook, Ji-won Park, Eun-Kyung Lee, et al. 2013.
<span>“PPtree: Projection Pursuit Classification Tree.”</span>
<em>Electronic Journal of Statistics</em> 7: 1369–86.
</div>
<div id="ref-da2017interactive" class="csl-entry">
Silva, Natalia da, Dianne Cook, and Eun-Kyung Lee. 2017.
<span>“Interactive Graphics for Visually Diagnosing Forest Classifiers
in r.”</span> <em>arXiv Preprint arXiv:1704.02502</em>.
</div>
<div id="ref-devtools" class="csl-entry">
Wickham, Hadley, and Winston Chang. 2015. <em>Devtools: Tools to Make
Developing r Packages Easier</em>. <a href="https://CRAN.R-project.org/package=devtools" class="external-link">https://CRAN.R-project.org/package=devtools</a>.
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Natalia da Silva.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
