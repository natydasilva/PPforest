---
title: "Projection pursuit classification random forest "
author: "N. da Silva, E. Lee & D. Cook"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
fig_caption: true
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{Vignette Title}
  \usepackage[utf8]{inputenc}
---


## Abstract

A random forest is an ensemble learning method, built on bagged trees. The bagging provides power for classification because it yields information about variable importance, predictive error and proximity of observations. This research adapts the random forest to utilize combinations of variables in the tree construction, which we call the projection pursuit classification random forest (`PPforest`). In a random forest each split is based on a single variable, chosen from a subset of predictors. In the `PPforest`, each split is based on a linear combination of randomly chosen variables. The linear combination is computed by optimizing a projection pursuit index, to get a projection of the variables that best separates the classes. The `PPforest` uses the `PPtree` algorithm, which fits a single tree to the data. Utilizing linear combinations of variables to separate classes takes the correlation between variables into account, and can outperform the basic forest when separations between groups occurs on combinations of variables. Two projection pursuit indexes, LDA and PDA, are used for `PPforest`.

##Introduction

The most common random forest implementations uses univariate decision trees like CART or C4.5.
These kinds of trees uses only one variable in each split and then define hyperplanes that are orthogonal to the axis. Sometimes we have data where the class can be separated by linear combinations and in these cases use a classifier which define hyperplanes that are oblique to the axis maybe do a better job.

`PPforest` package implements a random forest to utilize combinations of variables in the tree construction, which we call the projection pursuit classification random forest. For each split a random sample of variables is selected and a linear combination is computed by optimizing a projection pursuit index, to get a projection of the variables that best separates the classes.

The classification method presented here `PPforest` uses the `PPtree` algorithm implemented in R, which fits a single tree to the data. Utilizing linear combinations of variables to separate classes takes the correlation between variables into account, and can outperform the basic forest when separations between groups occur on combinations of variables. Two projection pursuit indexes, LDA and PDA, are used for `PPforest`.


###Motivation Example
Australian crab data set contains measurements on rock crabs of the genus Leptograpsus. There are 200 observations from two species (blue and orange) and for each specie (50 in each one) there are 50 males and 50 females. Class variable has 4 classes with the combinations of specie and sex (BlueMale, BlueFemale, OrangeMale and OrangeFemale). The data were collected on site at Fremantle, Western Australia. For each specimen, five measurements were made, using vernier calipers.

1. FL the size of the frontal lobe length, in mm
2. RW rear width, in mm
3. CL length of mid line of the carapace, in mm
4. CW maximum width of carapace, in mm
5. BD depth of the body; for females, measured after displacement of the abdomen, in mm

To visualize this data set we use a scatterplot matrix.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(PPforest2)
library(RColorBrewer)
library(GGally)
#library(rpart.plot)
library(gridExtra)
library(plotly)
```

```{r,fig.show='hold', fig.width = 6, fig.height = 6, echo=FALSE}
 load("../data/crab.rda")
a <- ggpairs(PPforest2::crab,
    columns= 2:6,
    ggplot2::aes(colour=Type,alpha=.3),
    lower=list(continuous='points'),
    axisLabels='none',
    upper=list(continuous='blank')
     )
```

<!-- In this figure we can see a strong, positive and linear association between the different variables and the different classes look like can be separated by linear combinations of variables. -->


<!-- ```{r, fig.show='hold',fig.width = 7 ,fig.height = 7, echo=FALSE, message=FALSE, warning=FALSE} -->


<!-- crab2 <- base::subset(crab, select=c(Type,RW,BD)) -->
<!-- grilla <- base::expand.grid(RW=seq(6,21,,50), BD=seq(6,22,,50)) -->

<!-- pptree <- PPtreeViz::PP.Tree.class(crab2[,1],crab2[,-1],"LDA") -->

<!-- ppred.crab <- PPtreeViz::PP.classify( pptree, test.data=grilla) -->
<!-- grilla$ppred <- ppred.crab[[2]] -->

<!-- rpart.crab <- rpart::rpart(Type ~ RW + BD, data=crab) -->
<!-- rpart.pred <- predict(rpart.crab, newdata = grilla, type="class") -->
<!-- ``` -->

<!-- ```{r, fig.show='hold',fig.width = 7, fig.height = 4, echo=FALSE, message=FALSE,warning=FALSE,eval=FALSE} -->

<!-- #To better understand the main characteristics of `PPtree` algorithm we will use a subset of crab data. We have selected two feature variables, RW and BD. With this reduced data set we run a `rpart` and a `PPtree` and we plot the decision boundaries for each tree. -->


<!-- # # p <- ggplot2::ggplot(data = grilla ) + ggplot2::geom_point(ggplot2::aes(x =RW, y = BD  , color = as.factor(ppred), ratio = 1),alpha = .50)+ ggplot2::scale_colour_brewer(type="qual",palette="Dark2") + ggplot2::theme_bw()  -->
<!-- # #    -->
<!-- # #  -->
<!-- # #  -->
<!-- # pl.pp <- p + ggplot2::geom_point(data = crab2, ggplot2::aes(x =RW , y = BD, group=Type,shape = Type, color=Type), size = I(3)  ) +  ggplot2::scale_shape_manual(name = "Class", labels = levels(crab2$Type),values = c(19, 18, 15, 17)) + ggplot2::scale_colour_brewer(type="qual",palette="Dark2") + ggplot2::theme_bw()+  ggplot2::scale_colour_manual(name = "Class", -->
<!-- #                       labels = levels(crab2$Type), -->
<!-- #                       values = c("#1B9E77" ,"#D95F02" ,"#7570B3" ,"#E7298A")) + -->
<!-- #   ggplot2::theme(legend.position = "bottom") -->

<!-- #  p2 <- ggplot2::ggplot(data = grilla ) + ggplot2::geom_point(ggplot2::aes(x =RW, y = BD  , color = as.factor(rpart.pred), ratio = 1), alpha = .5) + ggplot2::scale_colour_brewer(type="qual",palette="Dark2") + ggplot2::theme_bw()  -->
<!-- #    -->
<!-- #  -->
<!-- #  pl.rpart <- p2 + ggplot2::geom_point(data = crab2, ggplot2::aes(x =RW , y = BD, group=Type,shape = Type, color=Type), size = I(3)  ) +  ggplot2::scale_shape_manual(name = "Class", labels = levels(crab2$Type),values = c(19, 18, 15, 17)) + ggplot2::scale_colour_manual(name = "Class", -->
<!-- #                       labels = levels(crab2$Type), -->
<!-- #                       values =  c("#1B9E77" ,"#D95F02" ,"#7570B3" ,"#E7298A")) + ggplot2::theme(legend.position = "bottom") -->
<!-- #  -->
<!-- #  -->
<!-- #  -->
<!-- # gridExtra::grid.arrange(pl.rpart,pl.pp, nrow=1) -->

<!-- #The  left panel shows the `rpart` decision boundaries while the right panel shows the `PPtree` decision boundaries. -->
<!-- #Decision boundaries from  `rpart` are orthogonal to the axis and for this data set several splits are needed in order to capture the data structure. The structure of the decision boundaries generated by `rpart` doesn't corresponds with the data. On the other hand, `PPtree` the decision boundaries defined by `PPtree` follows the structure defined by the data. We can see that `PPtree` require less splits to capture the data structure. -->

<!-- ``` -->

<!-- ##Projection pursuit classification forest -->
<!--  `PPforest` implements a projection pursuit classification random forest. Projection pursuit classification trees  are used to build the forest, ( from `PPtreeViz` package ). `PPforest` adapts random forest to utilize combinations of variables in the tree construction. -->

<!-- `PPforest` is generated from `PPtree` algorithm. `PPtree` combines tree structure methods with projection pursuit dimensional reduction. -->

<!-- One important characteristic of PPtree is that treats the data always as a two-class system,  when the classes are more than two the algorithm uses a two step  projection pursuits optimization in every node split.  -->
<!-- Let  $(X_i,y_i)$ the data set, $X_i$ is a  p-dimensional vector of explanatory variables and  $y_i\in {1,2,\ldots G}$ represents class information with $i=1,\ldots n$. -->

<!-- In the first step optimize a projection pursuit index to find an optimal one-dimension projection $\alpha^*$ for separating all classes in the current data. With the projected data redefine the problem in a two class problem by comparing means, and assign a new label $G1$ or $G2$ to each observation, a new variable $y_i^*$ is created.  The new groups $G1$ and $G2$ can contain more than one original classes. Next step is to find an optimal one-dimensional projection $\alpha$, using $(X_i,y_i^*)$ to separate the two class problem $G1$ and $G2$. The best separation of $G1$ and $G2$ is determine in this step and the decision rule is defined for the current node, if $\sum_{i=1}^p \alpha_i M1< c$ then assign $G1$ to the left node else assign $G2$ to the right node, where $M1$ is the mean of $G1$. -->
<!-- For each groups we can repeat all the previous steps until $G1$ and $G2$ have only one class from the original classes. Base on this process to grow the tree, the depth of PPtree is at most the number of classes.   -->

<!-- Trees from `PPtree` algorithm are simple, they use the association between variables to find separation. If a linear boundary exists, `PPtree` produces a tree without misclassification. -->
<!-- One class is assigned only to one final node, depth of the tree is at most the number of classes. -->
<!-- Finally the projection coefficient of the node represents the variable importance. -->


<!-- ```{r,  fig.show='hold',fig.width = 7, fig.height = 4, echo=FALSE, message=FALSE} -->

<!-- # The following plots shows the tree structure from `rpart` and `PPtree`. Trees from `PPtree` are smaller and simpler #than trees from `rpart`. For this example `PPtree` present three splits while `rpart` tree did  13 splits.  -->

<!-- # pptree <- PPtreeViz::PP.Tree.class(crab[,1],crab[,-1],"LDA") -->
<!-- # rp <- rpart::rpart(Type~ .,data=crab) -->
<!-- #  -->
<!-- # pl2 <- rpart.plot::rpart.plot(rp) -->
<!-- # pl1 <- plot(pptree) -->


<!-- ``` -->


<!-- The PPforest uses the PPtree algorithm, which fits a single tree to the data. Utilizing linear combinations of variables to separate classes takes the correlation between variables into account, and can outperform the basic forest when separations between groups occur on combinations of variables. Two projection pursuit indexes, LDA and PDA, are used for PPforest. -->

<!-- Projection pursuit random forest algorithm description -->

<!-- 1. Let N the number of cases in the training set $\Theta=(X,Y)$, $B$ bootstrap samples from the training set are taking (samples of size N with replacement) -->
<!-- 2. For each bootstrap sample a `PPtree`  is grown to the largest extent possible $h(x, {\Theta_k})$. No pruning. This tree is grown using step 3 modification. -->
<!-- 3. Let M the number of input variables, a number of $m<<M$ variables are selected at random at each node and the best split based on a linear combination of these randomly chosen variables. The linear combination is computed by optimizing a projection pursuit index, to get a projection of the variables that best separates the classes. -->
<!-- 4.  Predict the classes of each case not included in the bootstrap sample and compute oob error. -->
<!-- 5.  Based on majority vote predict the class for new data. -->

<!-- ###Overview PPforest package -->
<!-- `PPforest` package implements a classification random forest using projection pursuit classification trees. The following table present all the functions in `PPforest` package.  -->

<!-- | Function |Description | -->
<!-- | ------ | ------ | -----: | -->
<!-- |PPforest|Runs a Projection pursuit random forest| -->
<!-- |predict.PPforest|Vector with predicted values from a PPforest object| -->
<!-- |ppf_importance| Plot a global measure of importance in PPforest.| -->
<!-- |pproxy_plot| Proximity matrix visualization| -->
<!-- |ppf_oob_error| OOB error visualization| -->
<!-- |var_select  |Index id for variables set, sample variables without replacement with constant sample proportion | -->
<!-- |train_fn |Index id for training set, sample in each class with constant sample proportion. | -->
<!-- |PPtree_split|Projection pursuit classification tree with random variable selection in each split| -->
<!-- |trees_pp| Projection pursuit trees for bootstrap samples| -->
<!-- |tree_ppred|Vector with predicted values from a PPtree object.| -->
<!-- |ppf_bootstrap| Draws bootstrap samples with strata option.| -->
<!-- |print.PPforest| Print PPforest object| -->

<!-- Also `PPforest` package includes some data set that were used to test the predictive performance of our method. The data sets included are: crab, fishcatch, glass, image, leukemia, lymphoma NCI60, parkinson and wine. -->

<!-- `train_fn`, `var_select`, `PPtree_split`, `trees_pp`, `tree_ppred` are auxiliary functions used in `PPforest`. -->
<!-- `train_fn` is a function that obtain stratified sample of the data, it returns a vector with the sample ids. This function has three arguments, data is a data frame with the complete data set, class is a character with the name of the class variable in my data set and size.p is the sample proportion we want. The sample is stratified by each class with the same proportion. -->

<!-- Here is an example how we can use this function: -->

<!-- ```{r} -->
<!-- training.id <- train_fn(data = crab, class= "Type", size.p = 2/3) -->
<!-- training.id -->
<!-- ``` -->
<!-- If we want all the selected id's we use `training.id$id` -->

<!-- `var_select` is a function that select at random the number of explanatory variables and returns a vector with the selected variables ids.  -->

<!-- `PPtree_split` this function implements a projection pursuit classification tree with random variable selection in each split. This function returns a `PPtreeclass` object. -->
<!-- To use this function we need to specify a formula describing the model to be fitted response\~predictors (`form`),  -->
<!-- `data` is a data frame with the complete data set. Also we need to specify the method `PPmethod`, it is the index to use for projection pursuit: 'LDA', 'PDA', 'Lr', 'GINI', and 'ENTROPY' -->
<!-- `size.p` is the proportion of variables randomly sampled in each split. -->
<!-- ` r` is a positive integer value, it is the power in Lr index. The default value is 1. -->
<!-- `lambda` penalty parameter in PDA index and is between 0 to 1 .  -->
<!-- `energy` optimization parameter for projection pursuit. Is the parameter for the probability to take a new projection. -->
<!-- `maxiter` number of maximum iterations.  -->

<!-- The following example fits a projection pursuit classification tree constructed using 0.6 of the variables (3 out of 5) in each node split. We selected `LDA` method. -->
<!-- ```{r} -->
<!-- Tree.crab <- PPtree_split("Type~.", data = crab, PPmethod = "LDA", size.p = 0.6) -->
<!--  Tree.crab -->
<!-- ``` -->

<!-- `trees_pp` this function grow a `PPtree_split` for each bootstrap sample. -->
<!-- This function returns a data frame with the results from `PPtree_split` for each bootsrap samples. -->

<!-- ```PPforest``` function runs a projection pursuit random forest. This function also has a data argument and class argument. Using this function we have the option to split the data in training and test using size.tr directly. `size.tr` is the size proportion of the training then the test proportion will be 1- `size.tr`. -->
<!-- The number of trees in the forest is specified using the argument `m`. The argument size.p is the sample proportion of the variables used in each node split, `PPmethod` is the projection pursuit index to be optimized there are two options LDA and PDA. Finally strata=TRUE indicates that the bootstrap samples are stratified by class variable.  -->

<!-- ```{r tidy=FALSE} -->
<!-- set.seed(146) -->
<!-- pprf.crab <- PPforest(data = crab, class = "Type", size.tr = 1, m = 100, -->
<!--                                 size.p =  .5,  PPmethod = 'LDA', strata = TRUE) -->

<!-- ``` -->

<!-- `PPforest` function returns the predicted values of the training data, training error, test error and predicted test values. Also there is the information about out of bag error for the forest and also for each tree in the forest. Bootstrap samples, output of all the trees in the forest from trees_pp function, proximity matrix and vote matrix, number of trees grown in the forest, number of predictor variables selected to use for splitting at each node. Confusion matrix of the prediction (based on OOb data), the training data and test data and vote matrix are also returned. -->

<!-- The printed version of a `PPforest` object follows the `randomForest` printed version to make them comparable. Based on confusion matrix, we can observe that the biggest error is for BlueMale class. Most of the wrong classified values are between BlueFemale and BlueMale. -->
<!-- ```{r} -->
<!--  pprf.crab -->
<!-- ``` -->
<!-- If we compare the results with the `randomForest` function for this data set the results are the following: -->

<!-- ```{r} -->
<!--   rf.crab <- randomForest::randomForest(Type~., data = crab, proximity = TRUE, ntree = 100) -->
<!--   rf.crab -->
<!-- ``` -->
<!-- We can see that for this data set the `PPforest` performance is much better than using `randomForest`. `PPforest` works well since the classes can be separated by linear combinations of variables.  -->
<!-- This is a clear case where oblique hyperplanes are more adequate in this case than hyperplanes horizontal to the axis. -->


<!-- ```{r,echo=FALSE} -->
<!--   #We can get the predicted values for training and test using the output of `PPforest` -->

<!--  #pred.training <- pprf.crab$prediction.training -->
<!--  #pred.test <- pprf.crab$prediction.test -->
<!--  #pred.test[1:10] -->
<!-- ``` -->
<!-- Some visualizations are possible in `PPforest`, because the PPforest is composed of many tree fits on subsets of the data, a lot of statistics can be calculated to analyze as a separate data set, and better understand how the model is working. -->
<!-- Some of the diagnostics of interest are: variable importance, OOB error rate, vote matrix and proximity matrix. -->

<!-- With a decision tree we can compute for every pair of observations the proximity matrix. This is a $nxn$ matrix where if two cases $k_i$ and $k_j$ are in the same terminal node increase their proximity by one, at the end normalize the proximities by dividing by the number of trees.  -->
<!-- To visualize the proximity matrix we use a heat map plot and an a scatter plot with information from multidimensional scaling method. -->

<!-- From `PPforest` object we can plot a heat map of the proximity matrix using the function `pproxy_plot`. -->
<!-- This function has tree arguments, `ppf` is a`PPforest` object, `type` an argument that specify if the plot is a heatmap or a MDS plot. -->
<!-- If the plot is a MDS plot the argument `k` defines the number of MDS layouts. -->
<!-- This function return an interactive plot based on `plotly` package. -->

<!-- ```{r,fig.show='hold',fig.width = 5 ,fig.height = 4, warning=FALSE} -->

<!-- pproxy_plot(pprf.crab, type = "heat") -->

<!-- ``` -->
<!-- In this plot we can see a heat map for the proximity matrix, we can observe that strong red color indicates that the observations are more similar. -->
<!-- The data are ordered by class (BlueFemale, BlueMale, OrangeFemale and OrangeMale), in the heat map we can observe a colored block diagonal structure, this means that the observations from the same class are similar here the same class were classified most of the time in the correct class, but also we can observe that observations from BlueMale and BlueFemale are similar too then some data were classified in the incorrect class. -->



<!-- Additionally `pproxy_plot` can be used to plot the MDS using proximity matrix information. -->

<!-- If we select k=2 the output plot is as follows: -->

<!-- ```{r,fig.show='hold',fig.width = 6 ,fig.height = 4, warning = FALSE} -->
<!-- pproxy_plot(pprf.crab, type="MDS", k =2 ) -->
<!-- ``` -->

<!-- We can observe a spatial separation between classes. Orange (male and female) are more separated than Blue (male and female). -->

<!-- If we select k>2,  we can observe that using two dimensions is enough to see the spatial separation. -->

<!-- ```{r,fig.show='hold',fig.width = 6 ,fig.height = 6, warning=FALSE} -->
<!-- pproxy_plot(pprf.crab, type="MDS",k = 3) -->
<!-- ``` -->
<!-- Another possible visualization in `PPforest` package is for the importance measure. -->

<!-- The variable importance for the group separation can be measured by the projection coefficients in each individual tree. Based on these coefficients we can examine how the classes are separated and which variables are more relevant for the separation.  -->

<!--  In `PPtree` the projection coefficient of each node represent the importance of variables to class separation in each node. Since in `PPforest` we have `m` trees we can define a global importance measure.  For this global importance measure we need to take into account the importance in each node and combine the results for all the trees in the forest. The importance measure of `PPforest` is a  weighted mean of the absolute value of the projection coefficients across all nodes in every tree. The weights are  the projection pursuit index in each node, and 1-the out of bag error of each tree. -->

<!-- `ppf_importance` has four arguments, data (data frame with the complete data set), class (a character with the name of the class variable), global ( logical that indicate is the importance measure is global or not) and weight (logical argument that indicates if the importance measure is weighted or not). -->


<!-- Using the `ppf_importance` function we can plot the weighted global importance measure in `PPforest`. -->

<!-- ```{r, fig.show='hold',fig.width = 5 ,fig.height = 4, warning=FALSE} -->
<!-- ppf_importance(data = crab, class = "Type", pprf.crab, global = TRUE, weight = TRUE)  -->
<!-- ``` -->

<!-- We can see that the most important variable in this example is RW while the less important is BD. -->
<!-- An importance measure for each node also is available. -->

<!-- ```{r, fig.show='hold',fig.width = 5 ,fig.height = 4, warning=FALSE} -->
<!-- ppf_importance(data = crab, class = "Type", pprf.crab, global = FALSE, weight = TRUE)  -->
<!-- ``` -->
<!-- We can see that in all the cases the importance variable order are the same. -->

<!-- Finally a cumulative out of error plot can be done using `ppf_oob_error` -->
<!-- This function has three arguments, ppf (`PPforest` object), nsplit1 (number, increment of the sequence where cumulative oob error rate is computed in the  1/3 trees) and nsplit2 (number, increment of the sequence where cumulative oob error rate is computed in the  2/3 trees). -->


<!-- ```{r, fig.show='hold',fig.width = 6 ,fig.height = 4, warning=FALSE} -->
<!--  ppf_oob_error(pprf.crab, nsplit1 = 3, nsplit2 = 15) -->
<!-- ``` -->

<!-- The oob-error rate decrease when we increase the numbers of trees but gets constant with less than 70 trees.  -->

<!-- ##NCI60 example -->

<!-- Gene expression data set, contains 61 observations and 30 feature variables. Class variable has 8 different tissue types, 9 breast, 5central nervous system (CNS), 7 colon, 6 leukemia, 8 melanoma, 9 non- small-cell lung carcinoma (NSCLC), 6 ovarian and 9 renal. There are 6830 genes. -->

<!-- ```{r, echo=FALSE,message=FALSE} -->
<!--  load("../data/NCI60.rda") -->
<!-- # PPtreeViz::PPopt.Viz(PPtreeViz::PDAopt(NCI60[, 1], as.matrix(NCI60[, -1]), q = 1)) -->

<!-- ``` -->


<!-- Since this data set present a considerable big number of predictive variables we can select some of them to explore using a scatterpot matrix. We can see in this plot that the data are correlated and the green class (leukemia) is separated from the other classes. -->

<!-- ```{r, fig.show='hold', fig.width = 6, fig.height = 6, echo=FALSE, warning=FALSE} -->
<!--   ggpairs(NCI60,  -->
<!--     columns= 27:31, -->
<!--     ggplot2::aes(colour = Type, alpha = .3), -->
<!--     lower=list(continuous = 'points'),  -->
<!--     axisLabels = 'none',   -->
<!--     upper=list(continuous = 'blank')  -->
<!--      )  -->

<!-- ``` -->

<!-- ```{r, fig.show='hold', fig.width = 10, fig.height = 5, echo=FALSE,warning=FALSE} -->
<!-- # gpd <-ggparcoord(data = NCI60, columns = 2:31, groupColumn = 1, order = "anyClass", showPoints = TRUE) + theme(axis.text.x = element_text(angle = 90), legend.position="bottom") -->
<!-- #  gpd -->
<!-- ``` -->


<!-- ```{r, fig.show='hold',fig.width = 7 ,fig.height = 4, message=FALSE,eval=FALSE,echo=FALSE} -->
<!-- #If we select q=2, `PPopt.Viz shows the best 2 dimensional projection coefficients and the scatter plot of the #projected data. We can observe that colon and leukemia are separable. -->
<!-- # library(ggplot2) -->
<!-- # PPtreeViz::PPopt.Viz(PPtreeViz::PDAopt(NCI60[, 1], as.matrix(NCI60[, -1]), q = 2)) -->
<!-- ``` -->

<!-- A `PPforest` is performed using the complete data set, 70 trees and 5 variables in each node split (size.p=0.15) are used. -->

<!-- ```{r tidy=FALSE,warning=FALSE} -->
<!--  set.seed(123) -->
<!--  pprf.nci60 <- PPforest(data = NCI60, class = "Type", size.tr = 1, m = 70, size.p = .15,  -->
<!--                         PPmethod = 'PDA', strata = TRUE, lambda=.5) -->
<!--  pprf.nci60 -->
<!-- ``` -->


<!-- We can compare the `PPforest` performance with `randomForest` using the same number of trees and the same random selected variables in the node split. -->
<!-- ```{r,warning=FALSE} -->
<!-- set.seed(123) -->
<!--  rf.nci60 <- randomForest::randomForest(Type~., data = NCI60, proximity = TRUE,ntree=70) -->
<!-- rf.nci60 -->
<!-- ``` -->

<!-- In this case `PPforest` also shows a better performance than `randomForest`. The out-of-bag error using `PPforest` was $24\%$ while using random forest was $47\%$. -->

<!-- We can also plot the MDS of the proximity matrix using the function `pproxy_plot`. -->
<!-- If we select k=2 we can see a block diagonal structure with 8 blocks that are related with the order of the classes (Breast, CNS, Colon, Leukemia, Melanoma, NSCLC, Ovarian and Renal). Colon and leukemia are classes that are different to the rest of the classes these two classes are classified without error.   -->
<!-- ```{r,v21,fig.show='hold',fig.width = 5 ,fig.height = 4,echo=FALSE,warning=FALSE} -->
<!-- pproxy_plot( pprf.nci60, type="heat" ) -->
<!-- ``` -->


<!-- In MDS plot we can also see that Leukemia and Colon are separate from the other classes. -->

<!-- ```{r, fig.show='hold',fig.width = 6 ,fig.height = 4,echo=FALSE,warning=FALSE} -->
<!-- pproxy_plot(pprf.nci60, type="MDS",k = 3) -->
<!-- ``` -->


<!-- The importance variable plot shows that the most important variables are Gene25 to Gene12 while Gene30 and Gene16 are the less important for the classification `PPforest`. -->
<!-- ```{r,fig.show='hold',fig.width = 5 ,fig.height = 4, echo = FALSE, warning = FALSE} -->
<!-- ppf_importance(data = NCI60, class = "Type", pprf.nci60, global = TRUE, weight = TRUE)  -->
<!-- ``` -->

<!-- The cumulative oob error plot shows that the overall cumulative error is reduced when we increase the number of trees, in this case we can increase the number of trees to have a reduction in the error. -->



<!-- ```{r,v24, fig.show='hold',fig.width = 6 ,fig.height = 4,warning=FALSE} -->
<!-- ppf_oob_error(pprf.nci60, nsplit1 = 5, nsplit2 = 10)    -->
<!-- ``` -->

